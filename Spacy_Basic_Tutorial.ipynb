{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy Basic Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "US2RP3vtxoyv"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"\"\"Dr. strange ordered samosas, ravioli etc. for his lunch.\"\"\")\n",
        "for sentences in doc.sents:\n",
        "  print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azRVU96myhMG",
        "outputId": "bb676ab7-1b83-4c40-fd5b-f21f5aa33f58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. strange ordered samosas, ravioli etc.\n",
            "for his lunch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`spacy.load(\"en_core_web_sm\")` *loads a complete pipeline that can work around english language and that contains tokenizer(sentence and word),lemmatizer,stemmer i.e. complete package........*"
      ],
      "metadata": {
        "id": "czQVEYd3XsI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp1 = spacy.blank('en')\n",
        "doc1 = nlp1(\"\"\"Dr. strange ordered samosas, ravioli etc. for his lunch.\"\"\")\n",
        "for sentences in doc1.sents:\n",
        "  print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "VJFFCh33USQH",
        "outputId": "d7ab884b-ea33-48cf-a3ac-0d28c98e270e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c7ad1a385226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Dr. strange ordered samosas, ravioli etc. for his lunch.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`spacy.blank('en')` *loads a simple class that can only do word tokenizer and for other purposes you have to explicitly download some other package.*"
      ],
      "metadata": {
        "id": "lgNq0pQzYIpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38LB5jDGXQIl",
        "outputId": "96bfb31b-21d6-4b75-b58d-c70920de765b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "strange\n",
            "ordered\n",
            "samosas\n",
            ",\n",
            "ravioli\n",
            "etc\n",
            ".\n",
            "for\n",
            "his\n",
            "lunch\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*see it only has word tokenizer*"
      ],
      "metadata": {
        "id": "zLXl1hA8YpBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\"\n",
        "\n",
        "doc2 = nlp(text)\n",
        "wd = doc2[0] #Accessing the token by index numbers"
      ],
      "metadata": {
        "id": "gHDIzoKrVif7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7O_BJyQHBdz",
        "outputId": "b8050c33-18c0-4067-d7a8-0814016ba1ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(wd) #To see what are the functions possible to perform in the token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvT85aArV-Aj",
        "outputId": "f51ad4d9-062e-4511-d8dc-b5f524859c8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_extension',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'string',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(\"There are 2 oranges and three apples\")\n",
        "for token in doc2:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "-sC_yyzbWID-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0d02e8-fda9-45a2-cbc2-63bfbfc78ded"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla\n",
            "'s\n",
            "gross\n",
            "cost\n",
            "of\n",
            "operating\n",
            "lease\n",
            "vehicles\n",
            "in\n",
            "FY2021\n",
            "Q1\n",
            "was\n",
            "$\n",
            "4.85\n",
            "billion\n",
            ".\n",
            "BMW\n",
            "'s\n",
            "gross\n",
            "cost\n",
            "of\n",
            "operating\n",
            "vehicles\n",
            "in\n",
            "FY2021\n",
            "S1\n",
            "was\n",
            "$\n",
            "8\n",
            "billion\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = doc3[2] #Assigning the token 2 to e\n",
        "f = doc3[5] #Assigning the token three to f"
      ],
      "metadata": {
        "id": "lJOoXBouW_q5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e.like_num #Checking whether the token e is a number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OpFExnmXItE",
        "outputId": "0723582f-6e11-4737-ed3f-75685d995f33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.is_alpha #Checking whether the token e is a alphabet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMfD91fyXLki",
        "outputId": "504e2bad-cd8d-49ae-e8b2-8fa38ebad0db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.is_alpha #Checking whether the token f is a alphabet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVCm1H3oXNev",
        "outputId": "23544e0d-6d05-4130-c37e-5179dac41ee5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f.like_num #Checking whether the token f is a number (since three also denotes the number it returns true in both cases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PM0TC5tXPTh",
        "outputId": "7b83d10a-e7a9-46b8-e5a2-e8030562b8b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information \n",
        "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
        "for tesla related news,\n",
        "https://twitter.com/teslarati\n",
        "https://twitter.com/dummy_tesla\n",
        "https://twitter.com/dummy_2_tesla\n",
        "'''\n",
        "\n",
        "doc4 = nlp(text)\n",
        "url = []\n",
        "\n",
        "for token in doc4:\n",
        "  if token.like_url: #Checking whether the token is a url and appending into the list\n",
        "    url.append(token.text)\n",
        "\n",
        "url"
      ],
      "metadata": {
        "id": "ucwZHIbnzzQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd1af92-94d3-497b-e162-af1274a5ee0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://twitter.com/elonmusk',\n",
              " 'https://www.tesla.com/.',\n",
              " 'https://twitter.com/teslarati',\n",
              " 'https://twitter.com/dummy_tesla',\n",
              " 'https://twitter.com/dummy_2_tesla']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
        "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
      ],
      "metadata": {
        "id": "iyEua66iGLgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc5 = nlp(text)\n",
        "websites = []\n",
        "for token in doc5:\n",
        "  if token.like_url:\n",
        "    websites.append(token.text)\n",
        "\n",
        "websites"
      ],
      "metadata": {
        "id": "bPDwCHEAY9qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc6f8df-bc9e-49d8-c8ec-64aa3113aba3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://www.data.gov/',\n",
              " 'http://www.science',\n",
              " 'http://data.gov.uk/.',\n",
              " 'http://www3.norc.org/gss+website/',\n",
              " 'http://www.europeansocialsurvey.org/.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
        "\n",
        "two $\n",
        "\n",
        "500 €"
      ],
      "metadata": {
        "id": "f-yvU2QfGmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "doc6 = nlp(transactions)\n",
        "\n",
        "for i in range(len(doc6)):\n",
        "  if doc6[i].is_currency:\n",
        "    print(doc6[i-1].text , doc6[i].text) "
      ],
      "metadata": {
        "id": "QDv78LtJ1xQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7bf489-e6ef-4de1-b9c6-0a2f1386e2eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two $\n",
            "500 €\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email = []\n",
        "\n",
        "mail = \"There are 2 mails you can contact me by one is haarks2019@gmail.com and another one is my personal mail abs@ghi.com\"\n",
        "doc7 = nlp(mail)\n",
        "\n",
        "for token in doc7:\n",
        "  if token.like_email:  #token.like_email to check whether the token is a mail\n",
        "    email.append(token.text)\n",
        "\n",
        "email"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TBKvl_WRbu7",
        "outputId": "2a3c32b5-bcb3-48e0-c519-c99e24b06951"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haarks2019@gmail.com', 'abs@ghi.com']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem = \"flying fly flew playing played studying study studied studies\"\n",
        "doc8 = nlp(lem)\n",
        "\n",
        "for token in doc8:\n",
        "  print(token.text,\"==>\",token.lemma_)  #token.lemma_ to lemmatize the token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-fOgq_-SAQf",
        "outputId": "53cfa8f4-ac2f-4254-c656-cde859a22c38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flying ==> fly\n",
            "fly ==> fly\n",
            "flew ==> fly\n",
            "playing ==> play\n",
            "played ==> play\n",
            "studying ==> study\n",
            "study ==> study\n",
            "studied ==> study\n",
            "studies ==> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "doc9 = nlp(text)\n",
        "for ent in doc9.ents:\n",
        "  print(ent, \"||\", ent.label_,\"||\", spacy.explain(ent.label_)) #Named Entity Recognition (NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgZ24sSkvfmg",
        "outputId": "abe5723b-a3c8-4824-bfbb-0ccacc0b303b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the United Kingdom || GPE || Countries, cities, states\n",
            "Two || CARDINAL || Numerals that do not fall under another type\n",
            "the General Social Survey || ORG || Companies, agencies, institutions, etc.\n",
            "the European Social Survey || ORG || Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc10 = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "\n",
        "for token in doc10:\n",
        "  print(token, \"||\", token.pos_, \"||\",spacy.explain(token.pos_)) #Part of Speech tagging "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wq1WOaTvxBN",
        "outputId": "f54b22ad-acaa-45c8-fa07-50ad7760961d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captain || PROPN || proper noun\n",
            "america || PROPN || proper noun\n",
            "ate || VERB || verb\n",
            "100 || NUM || numeral\n",
            "$ || SYM || symbol\n",
            "of || ADP || adposition\n",
            "samosa || NOUN || noun\n",
            ". || PUNCT || punctuation\n",
            "Then || ADV || adverb\n",
            "he || PRON || pronoun\n",
            "said || VERB || verb\n",
            "I || PRON || pronoun\n",
            "can || VERB || verb\n",
            "do || AUX || auxiliary\n",
            "this || DET || determiner\n",
            "all || DET || determiner\n",
            "day || NOUN || noun\n",
            ". || PUNCT || punctuation\n"
          ]
        }
      ]
    }
  ]
}